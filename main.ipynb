{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MRyo-ie/DataMining_Report3_Lv4_NLP_Models/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytKWwxITuiOK",
        "colab_type": "text"
      },
      "source": [
        "# Report3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv_tdY_kpkwW",
        "colab_type": "text"
      },
      "source": [
        "## Level 4: 文書分類せよ。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQI8s3RxSO36",
        "colab_type": "code",
        "outputId": "4df677fb-fdcb-4772-ee1e-cf1684443eee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 自分の Google Drive をマウント\n",
        "# https://qiita.com/uni-3/items/201aaa2708260cc790b8#drive内のディレクトリをマウントする220180920\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "#@markdown ### GitHubリポジトリをcloneして保存するパス （Google Drive上）\n",
        "REPOSITORY_ROOT_DIR = 'Colab Notebooks/learning/old_nlp_techs' #@param {type: \"string\"}"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMga8fELbUHX",
        "colab_type": "text"
      },
      "source": [
        "### << 実行手順 >>\n",
        "\n",
        "1.  ↑ の、このボタンを押す　→　　![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)\n",
        "\n",
        "1. ↖︎（左上）のどこかに「ドライブに保存」があるので、それをクリックして、  \n",
        "  自分の Google Drive にこのファイルをコピーする。\n",
        "\n",
        "1. ↑ の変数を設定する。\n",
        "  - GitHub リポジトリをクローンする場所（GoogleDrive上の好きな場所）のパスを入力。\n",
        "\n",
        "1. 実行。  \n",
        "  - ↑で GoogleDrive をマウントするので、（英語の）指示に従って、  \n",
        "    「ログイン」→「パスコードをコピー」→「四角枠に貼り付けしてEnter」\n",
        "\n",
        "1. 後は勝手に実行してくれるはず"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd7zfCZH2YJ-",
        "colab_type": "text"
      },
      "source": [
        "### 【準備】 パスなどの設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cefTwUbsegWV",
        "outputId": "b59475fd-4aae-48c6-ce8d-27dbe789c393",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "REPOSITORY_URL = 'https://github.com/MRyo-ie/DataMining_Report3_Lv4_NLP_Models.git'\n",
        "DATA_DL_URL = \"https://www.rondhuit.com/download/ldcc-20140209.tar.gz\"\n",
        "GDRIVE_ROOT = '/content/gdrive/My Drive/'\n",
        "REPO_PATH = os.path.join(GDRIVE_ROOT, REPOSITORY_ROOT_DIR, REPOSITORY_URL.split('/')[-1][:-4])\n",
        "\n",
        "# よく使うパス\n",
        "DATA_DIR = os.path.join(REPO_PATH, 'datas', 'articles_livedoor')\n",
        "TMP_DIR = os.path.join(DATA_DIR, 'tmp')\n",
        "FILE_PATH = os.path.join(DATA_DIR, 'tmp', DATA_DL_URL.split('/')[-1])\n",
        "\n",
        "print(\"\"\"\n",
        "------------------------------------------------\n",
        "<<<  パス一覧  >>>\n",
        "REPO_PATH = {}\n",
        "\n",
        "DATA_DL_URL = {}\n",
        "DATA_DIR = {}\n",
        "TMP_DIR = {}\n",
        "FILE_PATH = {}\n",
        "------------------------------------------------\"\"\".format(REPO_PATH, DATA_DL_URL, DATA_DIR, TMP_DIR, FILE_PATH))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "------------------------------------------------\n",
            "<<<  パス一覧  >>>\n",
            "REPO_PATH = /content/gdrive/My Drive/Colab Notebooks/learning/old_nlp_techs/DataMining_Report3_Lv4_NLP_Models\n",
            "\n",
            "DATA_DL_URL = https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
            "DATA_DIR = /content/gdrive/My Drive/Colab Notebooks/learning/old_nlp_techs/DataMining_Report3_Lv4_NLP_Models/datas/articles_livedoor\n",
            "TMP_DIR = /content/gdrive/My Drive/Colab Notebooks/learning/old_nlp_techs/DataMining_Report3_Lv4_NLP_Models/datas/articles_livedoor/tmp\n",
            "FILE_PATH = /content/gdrive/My Drive/Colab Notebooks/learning/old_nlp_techs/DataMining_Report3_Lv4_NLP_Models/datas/articles_livedoor/tmp/ldcc-20140209.tar.gz\n",
            "------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxpwrBz9zeyv",
        "colab_type": "text"
      },
      "source": [
        "### 【準備】 GitHub clone"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FAttiRhmUa3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "7615b93f-1243-48c9-fb02-ac8eee1c577b"
      },
      "source": [
        "if not os.path.exists(REPO_PATH):\n",
        "    repo_parent_path = os.path.join(GDRIVE_ROOT, REPOSITORY_ROOT_DIR)\n",
        "    %cd  \"$repo_parent_path\"\n",
        "    !git clone \"$REPOSITORY_URL\"  \"$REPO_PATH\"\n",
        "    %cd \"$REPO_PATH\"\n",
        "else:\n",
        "    # pull -f （と同じ意味のコマンド）\n",
        "    %cd \"$REPO_PATH\"\n",
        "    # 最新の状態に更新（強制）\n",
        "    ! git fetch origin master\n",
        "    ! git reset --hard origin/master"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/learning/old_nlp_techs/DataMining_Report3_Lv4_NLP_Models\n",
            "Host key verification failed.\n",
            "fatal: Could not read from remote repository.\n",
            "\n",
            "Please make sure you have the correct access rights\n",
            "and the repository exists.\n",
            "Checking out files: 100% (10/10), done.\n",
            "HEAD is now at d030aec Merge branch 'master' into GoogleColab-merge\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aZTAWBqnXR45"
      },
      "source": [
        "### 【準備】 データ\n",
        "今回は livedoor の記事データセットを使用。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "906T09c6azyW",
        "colab": {}
      },
      "source": [
        "import configparser\n",
        "import glob\n",
        "import os, sys\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "import tarfile\n",
        "from tqdm import tqdm\n",
        "from urllib.request import urlretrieve"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGvxmrZk6NZ6",
        "colab_type": "text"
      },
      "source": [
        "#### DL& 読み込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aJ4AQJNQeaxT",
        "colab": {}
      },
      "source": [
        "# !mkdir -p  \"$TMP_DIR\"\n",
        "# %cd  \"$TMP_DIR\"\n",
        "\n",
        "# if not os.path.exists(FILE_PATH):\n",
        "#     ! wget  \"$DATA_DL_URL\"\n",
        "# # 時間かかる。\n",
        "# !tar  -zxvf   'ldcc-20140209.tar.gz'\n",
        "# !ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LU0ddAwymB21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "8be792fe-0759-466f-eb78-c798f7ffdc9d"
      },
      "source": [
        "categories = [ \n",
        "    name for name \n",
        "    in os.listdir( os.path.join(TMP_DIR, \"text\") ) \n",
        "    if os.path.isdir( os.path.join(TMP_DIR, \"text\", name) ) ]\n",
        "\n",
        "categories = sorted(categories)\n",
        "categories"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dokujo-tsushin',\n",
              " 'it-life-hack',\n",
              " 'kaden-channel',\n",
              " 'livedoor-homme',\n",
              " 'movie-enter',\n",
              " 'peachy',\n",
              " 'smax',\n",
              " 'sports-watch',\n",
              " 'topic-news']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UY_enZFuWDYV"
      },
      "source": [
        "#### データの抽出・整備\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VzwBKkN5X_k_"
      },
      "source": [
        "- ラベル（記事の種類）一覧"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9XROrL41mHON",
        "outputId": "2169370b-ed64-44a1-e4e8-952c22a7e8f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "table = str.maketrans({\n",
        "    '\\n': '',\n",
        "    '\\t': '　',\n",
        "    '\\r': '',\n",
        "})\n",
        "\n",
        "def extract_txt(filename):\n",
        "    with open(filename) as text_file:\n",
        "        # 0: URL, 1: timestamp\n",
        "        text = text_file.readlines()[2:]\n",
        "        text = [sentence.strip() for sentence in text]\n",
        "        text = list(filter(lambda line: line != '', text))\n",
        "        return ''.join(text)\n",
        "\n",
        "%cd  \"$TMP_DIR/\""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/learning/old_nlp_techs/DataMining_Report3_Lv4_NLP_Models/datas/articles_livedoor/tmp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uwWtI1oLmLoL",
        "outputId": "ccf3bc39-c475-4866-d1a0-d304c9772ddd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        }
      },
      "source": [
        "%cd  \"$TMP_DIR/\"\n",
        "\n",
        "# 割と時間かかる。\n",
        "all_text = []\n",
        "all_label = []\n",
        "for cat in tqdm(categories):\n",
        "    files = glob.glob(os.path.join(\"text\", cat, \"{}*.txt\".format(cat)))\n",
        "    files = sorted(files)\n",
        "    body = [ extract_txt(elem).translate(table) for elem in files ]\n",
        "    label = [cat] * len(body)\n",
        "    \n",
        "    all_text.extend(body)\n",
        "    all_label.extend(label)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/9 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/learning/old_nlp_techs/DataMining_Report3_Lv4_NLP_Models/datas/articles_livedoor/tmp\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-e56243466f3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"{}*.txt\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mextract_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-e56243466f3e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"{}*.txt\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mextract_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-d68731255c53>\u001b[0m in \u001b[0;36mextract_txt\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtext_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# 0: URL, 1: timestamp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-2lrwW7OYHXC"
      },
      "source": [
        "- 特徴量とラベル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n7qEl5ZMmOYa",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame({'text' : all_text, 'label' : all_label})\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kgGmT86omSi9",
        "colab": {}
      },
      "source": [
        "df = df.sample(frac=1, random_state=23).reset_index(drop=True)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xdt92AsGYMqV"
      },
      "source": [
        "#### ファイルに保存 (機械学習モデル用)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyzhSfaSKvEM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "extract_dir = os.path.join(TMP_DIR, 'extracts')\n",
        "SentencePiece_train_dir = os.path.join(SP_MODEL_DIR, 'tmp')\n",
        "if not os.path.exists(os.path.join(SP_MODEL_DIR, 'tmp')):\n",
        "    ! mkdir  \"$extract_dir\"\n",
        "    ! mkdir  \"$SentencePiece_train_dir\"\n",
        "\n",
        "dpaths = {\n",
        "    'test' : os.path.join(extract_dir, \"test.tsv\"),\n",
        "    'val' : os.path.join(extract_dir, \"val.tsv\"),\n",
        "    'train' : os.path.join(extract_dir, \"train.tsv\"),\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2ruzfKDRmWZ7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "939fe8dc-f3c8-4b88-beb8-2da8486bbc02"
      },
      "source": [
        "# Sentence Piece 用の raw text データを作成\n",
        "df.to_csv( os.path.join(SentencePiece_train_dir, \"train.txt\"), sep=' ', index=False, header=False, columns=['text'] )\n",
        "! head -3  \"$SentencePiece_train_dir\"/train.txt\n",
        "\n",
        "# 言語モデル用の train, test, dev データファイルを作成\n",
        "df[:len(df) // 5].to_csv( dpaths['test'], sep='\\t', index=False )\n",
        "df[len(df) // 5:len(df)*2 // 5].to_csv( dpaths['val'], sep='\\t', index=False )\n",
        "df[len(df)*2 // 5:].to_csv( dpaths['train'], sep='\\t', index=False )\n",
        "P = dpaths['test']\n",
        "! head -3  \"$P\""
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-08679e00a45f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSentencePiece_train_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' head -3  \"$SentencePiece_train_dir\"/train.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 言語モデル用の train, test, dev データファイルを作成\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGkG5gbTCNss",
        "colab_type": "text"
      },
      "source": [
        "### Layer1 : 分かち書き（Tokenize）\n",
        "- 今回は **Sentence Piece** を使ってみる。  \n",
        "  https://www.madopro.net/entry/sentencepiece_rnn_lm\n",
        "- 一番上の２つを実行すれば、あとはここからでOK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bWaFwHS-UZg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# モデル関連のパス\n",
        "## Sentence Piece\n",
        "SP_MODEL_DIR = os.path.join(REPO_PATH, 'models', 'Tokenizer', 'SentencePiece')\n",
        "LANGUAGE_ROOT_DIR = os.path.join(REPO_PATH, 'models', 'LanguageModel')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G5tiI7wDmm0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "3acb127a-097e-436e-a703-65dc4d1bae47"
      },
      "source": [
        "! pip install sentencepiece"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\r\u001b[K     |▎                               | 10kB 25.5MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 4.5MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 2.9MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 3.6MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 4.3MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 5.0MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81kB 3.9MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 4.3MB/s eta 0:00:01\r\u001b[K     |███▏                            | 102kB 4.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112kB 4.8MB/s eta 0:00:01\r\u001b[K     |███▉                            | 122kB 4.8MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 4.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 143kB 4.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 153kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 163kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 174kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 184kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 194kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 204kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 215kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 225kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 235kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 245kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 256kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 266kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 276kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 286kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 296kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 307kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 317kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 327kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 337kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 348kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 358kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 368kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 378kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 389kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 399kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 409kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 419kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 430kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 440kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 450kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 460kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 471kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 481kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 491kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 501kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 512kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 522kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 532kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 542kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 552kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 563kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 573kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 583kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 593kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 604kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 614kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 624kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 634kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 645kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 655kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 665kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 675kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 686kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 696kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 706kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 716kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 727kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 737kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 747kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 757kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 768kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 778kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 788kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 798kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 808kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 819kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 829kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 839kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 849kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 860kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 870kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 880kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 890kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 901kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 911kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 921kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 931kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 942kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 952kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 962kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 972kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 983kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 993kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0MB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.0MB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 4.8MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh8S4bHDsF7w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8eb17a41-2031-44ef-9d7a-9689802d701a"
      },
      "source": [
        "%cd  \"$SP_MODEL_DIR\"\n",
        "!echo '[Info] Sentence Piece の学習を開始します。'\n",
        "if not os.path.exists('model'):\n",
        "    ! mkdir  'model'\n",
        "\n",
        "# SentencePiece を学習する：モデルを作る。\n",
        "import sentencepiece as spm\n",
        "# 5分くらいかかる。（コメントアウト外す）\n",
        "if not os.path.exists('model/m.model'):\n",
        "    spm.SentencePieceTrainer.Train('--input=tmp/train.txt --model_prefix=model/m --vocab_size=8000')\n",
        "print('[Info] 終了！')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/learning/old_nlp_techs/DataMining_Report3_Lv4_NLP_Models/models/Tokenizer/SentencePiece\n",
            "[Info] Sentence Piece の学習を開始します。\n",
            "[Info] 終了！\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znZdt0QFsF7z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "deb42486-e785-40eb-d4a3-4221e269e95f"
      },
      "source": [
        "# 学習したSentencePieceモデルを読み込む\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load('model/m.model')\n",
        "# こんな感じで 分かち書きできる。\n",
        "print( sp.EncodeAsPieces('吾輩は猫である') )\n",
        "# print( sp.EncodeAsPieces(df.at[1, 'text']) )"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁', '吾', '輩', 'は', '猫', 'である']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zE6Yq73O-1M",
        "colab_type": "text"
      },
      "source": [
        "### Layer2 : Embedding\n",
        "- 今回は、BoW+TF-IDF、共起行列、XLNet の予定。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls0_nr_M2xV2",
        "colab_type": "text"
      },
      "source": [
        "#### 事前処理\n",
        "- Token（単語など） に ID を振る。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7K3qguJ6zeJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "630a1faf-0480-4c9c-f4ed-fd3d50d8de42"
      },
      "source": [
        "sentences = []\n",
        "with open('tmp/train.txt', 'r', encoding='utf-8') as f:\n",
        "    sentences = f.read().split('\\n')\n",
        "print(sentences[7])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"ケーブルいらずのスマホバッテリ！情報満載の1日を振り返る【ITフラッシュバック】ThinkPadをNECで生産するニュースからケーブルいらずのバッテリー、その場で動画を共有するテクニック、315gの最軽量スキャナ、孫正義氏のTwitterまで、様々な記事が掲載された。情報満載の1日を振り返ってみよう。■ThinkPadをNECで生産へ 2012年秋より日本で試験生産【デジ通】レノボとNECパーソナルコンピュータは2011年7月より、NECレノボ・ジャパングループの元で運営されている。それから1年が経過し、ユーザーの目に見える部分では、レノボのサポートサービスをNECに委託し、レノボのIdeaCenterにNECのTV技術を用いた製品が発表されるなど、いつかの点でその成果が出始めている。さらに、一部の個人ユーザーに人気のThinkPadを、NECの米沢事業所で試験生産することを発表した。■Dockコネクターに直挿し！iPhoneやiPadにケーブルいらずのバッテリー【イケショップのレア物】今回紹介する「Universal PowerBank 6000 for Smartphone/Tablet」は、これ1台でかなり対応できるスグレモノのモバイルバッテリーなのである。■iPhoneで撮った動画を仲間で共有！その場で動画を共有するテクニック【知っ得！虎の巻】iPhoneを使えば、手軽に動画を撮影できる。プライベートシーンなら、家族やペットの撮影を楽しめる。ビジネスシーンなら、現場の様子を撮影してわかりやすく報告できる。もし、急ぎの報告が必要なら、いったんパソコンに取り込んでサーバーにアップするより、そのままYouTubeにアップして限定公開してみてはいかがだろう。■315gの最軽量スキャナも登場！PCレスでスキャンデータを活用できる「ADS-2500W」スマートフォンやタブレット端末などの普及やクラウドサービスの利用拡大などから、書類をデータ化して活用する機会が増え、ドキュメントスキャナーのニーズが高まっている。そうしたニーズにこたえるために、ブラザーが新製品を投入する。■電子ブックリーダー「kobo Touch」の実機が見られる！楽天が大手書店と連携した理由「お得に購入できる裏技あり！楽天の書籍端末が凄い5つの理由」でお伝えしたように、楽天の電子ブックリーダー「kobo Touch（コボタッチ）」が2012年7月2日より予約を開始した。電子ブックリーダーとしては安価な製品であるだけに、実機を見たい人もいるだろう。そうした人に朗報だ。■語れば語るほど、メッキが剥がれていく！孫社長「あきらめていないか？」ソフトバンクグループの代表 孫正義氏は、Twitterを通じて活発な発言をしている。そんな同氏の格言的な言葉がネットで話題となっている。それは、突然のつぶやきから始まった。■電子書籍に関連した記事を読む・電子書籍では何が重要なのか？ 本格化する日本の電子書籍【デジ通】・お得に購入できる裏技あり！楽天の書籍端末が凄い5つの理由・楽天が電子書籍サービスを開始？　「Kobo」のサイトを新たに開設・最新情報も見逃すな！ 大人気アニメ「宇宙兄弟」公式アプリ！【iPhoneでチャンスを掴め】・クリエイターと読者が創るソーシャル時代の電子書籍サービス「Yomuca」Transcend SDHCカード 16GB Class10 永久保証 [フラストレーションフリーパッケージ (FFP)] TS16GSDHC10Eトランセンド・ジャパン販売元：Amazon.co.jpクチコミを見る\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39VcxWOx926O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3cce9581-972a-443e-b9cc-598ac13f3fc4"
      },
      "source": [
        "%cd  \"$LANGUAGE_ROOT_DIR\"\n",
        "\n",
        "import pre\n",
        "from tqdm import tqdm\n",
        "\n",
        "# data_w2i を作る。\n",
        "data_w2i = {}\n",
        "for s in tqdm(sentences):\n",
        "    tknd_s = sp.EncodeAsPieces(s)\n",
        "    data_w2i = pre.identify_words(tknd_s, data_w2i)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/learning/old_nlp_techs/DataMining_Report3_Lv4_NLP_Models/models/LanguageModel\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7368/7368 [00:05<00:00, 1295.00it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyH26N9X17mO",
        "colab_type": "text"
      },
      "source": [
        "#### BoW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZ8ZIXr62wk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Bag_of_Words import BoW\n",
        "\n",
        "# 文（章）の list を渡すと、それらを BoW に変換した list を返す。\n",
        "def build_BoW_vecs(raw_sentences, data_w2i):\n",
        "    bow = BoW(data_w2i)\n",
        "    print('[Info] Tokenize中...')\n",
        "    tknzd_sentences = [sp.EncodeAsPieces(s) for s in raw_sentences]\n",
        "    return np.array([bow.embed_BoW(tknz_s) for tknz_s in tknzd_sentences])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0CftRPs_XnC",
        "colab_type": "text"
      },
      "source": [
        "#### co-matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-fE18-v1xxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hySomdSURqrp"
      },
      "source": [
        "### Layer3 : 分類モデル\n",
        "- 今回は、SVM で行う。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5w40n7Y9nx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Embedding Layer のアルゴリズムを選ぶ。\n",
        "Embed_layer = \"BoW\"  #@param ['BoW', 'co-matrix', 'XLNet']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29gaZeyBJQIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "extract_dir = os.path.join(TMP_DIR, 'extracts')\n",
        "dpaths = {\n",
        "    'test' : os.path.join(extract_dir, \"test.tsv\"),\n",
        "    'val' : os.path.join(extract_dir, \"val.tsv\"),\n",
        "    'train' : os.path.join(extract_dir, \"train.tsv\"),\n",
        "}\n",
        "\n",
        "# P = dpaths['test']\n",
        "# ! head -3  \"$P\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QUcyca1JQ1D",
        "colab_type": "text"
      },
      "source": [
        "#### train\n",
        "##### データ読み込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G_MVIGFAdN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.utils import resample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpV5PK2K8s-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###  train.tsv から データを読み込み  ###\n",
        "def get_datas(mode):\n",
        "    df = pd.read_csv(dpaths[mode], sep='\\t')\n",
        "    return df['text'].values, df['label'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9o4mQrR-WDis",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "63c606c3-7891-4ca0-e832-473ee12609fa"
      },
      "source": [
        "raw_datas, raw_labels = get_datas('train')\n",
        "print(type(raw_datas), len(raw_labels))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> 4421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVbhydrcBB7i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "912ca5a8-024e-448f-f0b6-0dc101f300ec"
      },
      "source": [
        "# データをシャッフル\n",
        "raw_datas, raw_labels = resample(raw_datas, raw_labels, n_samples=len(raw_labels))\n",
        "print(\"DataNum: \", len(raw_datas), '\\n', raw_labels)\n",
        "print(\"DataNum: \", len(datas),  '\\n', labels)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataNum:  4421 \n",
            " ['sports-watch' 'dokujo-tsushin' 'sports-watch' ... 'dokujo-tsushin'\n",
            " 'livedoor-homme' 'livedoor-homme']\n",
            "DataNum:  4421 \n",
            " ['peachy' 'it-life-hack' 'kaden-channel' ... 'movie-enter'\n",
            " 'dokujo-tsushin' 'it-life-hack']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7L-aNwP8A4Sk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "68e4a5c7-c373-4135-f45e-0a8759a52e61"
      },
      "source": [
        "###  ラベルを数値化  ###\n",
        "label_list = [ \n",
        "    name for name \n",
        "    in os.listdir( os.path.join(TMP_DIR, \"text\") ) \n",
        "    if os.path.isdir( os.path.join(TMP_DIR, \"text\", name) ) ]\n",
        "print(label_list)\n",
        "\n",
        "# 変換器（data_w2i） を作成\n",
        "label_w2i = {}\n",
        "for i, w in enumerate(name_list):\n",
        "    label_w2i[w] = i\n",
        "print(label_w2i)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['dokujo-tsushin', 'livedoor-homme', 'kaden-channel', 'smax', 'topic-news', 'peachy', 'movie-enter', 'it-life-hack', 'sports-watch']\n",
            "{'dokujo-tsushin': 0, 'livedoor-homme': 1, 'kaden-channel': 2, 'smax': 3, 'topic-news': 4, 'peachy': 5, 'movie-enter': 6, 'it-life-hack': 7, 'sports-watch': 8}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMtAysmvCo8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_labels(raw_labels, label_w2i:dict):\n",
        "    return np.array([label_w2i[w] for w in raw_labels])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToPEgYor58pN",
        "colab_type": "text"
      },
      "source": [
        "##### Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSMGB_9p50dd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "e629fa1a-51df-4293-815b-8f484345ae7c"
      },
      "source": [
        "# 生の文（章）から、文（章）ベクトルに変換する。\n",
        "datas = None\n",
        "if Embed_layer == 'BoW':\n",
        "    datas = build_BoW_vecs(raw_datas, data_w2i)\n",
        "\n",
        "print('[Info] 完了！')\n",
        "print(datas)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Info] Tokenize中...\n",
            "[Info] 完了！\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [2. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [2. 0. 0. ... 0. 0. 0.]\n",
            " [8. 4. 0. ... 0. 0. 0.]\n",
            " [3. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuOpJfilApe9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c3ca24b-e743-439d-9dc7-fa3a7a83ad48"
      },
      "source": [
        "# ラベルを変換\n",
        "labels = conv_labels(raw_labels, label_w2i)\n",
        "print(labels)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[8 0 8 ... 0 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM_RRZ7H2soF",
        "colab_type": "text"
      },
      "source": [
        "##### 学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU00jjWEgxFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##  パラメータ関係  ##\n",
        "from sklearn.svm import LinearSVC\n",
        "# SVC のハイパーパラメータ\n",
        "svc = LinearSVC(C=1.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7Nc3Sy626wN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train\n",
        "print(\"****START***********************\")\n",
        "svc.fit(data_vecs, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6cAaozuAPIn0"
      },
      "source": [
        "#### SVM で分類"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABs7lkyrHPS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: UTF-8\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "from matplotlib import pylab\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "import glob\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def read_csv(path):\n",
        "    df = pd.read_csv(path, index_col=0)\n",
        "    print(df.columns)\n",
        "    return np.array(X),np.array(y)\n",
        "\n",
        "def normalisation(cm):\n",
        "    new_cm = []\n",
        "    for line in cm:\n",
        "        sum_val = sum(line)\n",
        "        new_array = [float(num)/float(sum_val) for num in line]\n",
        "        new_cm.append(new_array)\n",
        "    return new_cm\n",
        "\n",
        "def plot_confusion_matrix(cm, name_list_org, title):\n",
        "    name_list = list(map(lambda x: x.replace('_', '\\n'), name_list_org))\n",
        "    print(name_list)\n",
        "    df = pd.DataFrame(cm, index=name_list, columns=name_list) #データフレーム化\n",
        "\n",
        "    plt.clf()\n",
        "    plt.cla()\n",
        "    fig, ax = plt.subplots(figsize=(12, 9)) \n",
        "    print('df : {}'.format(df))\n",
        "    #ax.set_ylim(len(cm), 0)\n",
        "    sns.heatmap(df, square=True, cmap='Blues', vmin=0, vmax=1.0, annot=True,)\n",
        "    pylab.title(title)\n",
        "    pylab.xlabel('Predict class')\n",
        "    pylab.ylabel('True class')\n",
        "\n",
        "    filename = title + \"_reslt.png\"\n",
        "    plt.savefig(filename)\n",
        "    #plt.close()\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "# TEST\n",
        "prediction = svc.predict(datas[:test_num])\n",
        "# 混同行列（図）で、結果を出力\n",
        "## plot\n",
        "cm = confusion_matrix(labels[:test_num],prediction)\n",
        "print(normalisation(cm))\n",
        "d_rate_str = \" ({}% → l:t=({}:{}))\".format(test_rate_p, len(datas) - test_num, test_num)\n",
        "plot_confusion_matrix(normalisation(cm), name_list, \"SEIYU classifer\"+d_rate_str)\n",
        "\n",
        "\n",
        "\n",
        "###  声感情分類  ###\n",
        "# Labels (Data Directory)\n",
        "name_list = [\"uemura_happy\", \"uemura_normal\", \"uemura_angry\"]\n",
        "#name_list = [\"tsuchiya_happy\", \"tsuchiya_normal\", \"tsuchiya_angry\"]\n",
        "#name_list = [\"fujitou_happy\", \"fujitou_normal\", \"fujitou_angry\"]\n",
        "\n",
        "# Read Datas\n",
        "datas,labels = read_mfcc(name_list, base_dir)\n",
        "# Suffle Datas\n",
        "# n_samples : Number of samples to generate.\n",
        "datas,labels = resample(datas, labels, n_samples=len(labels))\n",
        "print(\"DataFileNum:\" + str(len(datas)))\n",
        "test_num = int(len(datas) * split_rate_test)\n",
        "\n",
        "# TRAINING\n",
        "svc.fit(datas[test_num:], labels[test_num:])\n",
        "\n",
        "# TEST\n",
        "prediction = svc.predict(datas[:test_num])\n",
        "## plot\n",
        "cm = confusion_matrix(labels[:test_num],prediction)\n",
        "print(normalisation(cm))\n",
        "d_rate_str = \" ({}% → l:t=({}:{})\".format(test_rate_p, len(datas) - test_num, test_num)\n",
        "plot_confusion_matrix(normalisation(cm), name_list, \"SEIYU emotion classifer\"+d_rate_str)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWqonvK-Fyet",
        "colab_type": "text"
      },
      "source": [
        "### その他"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJk65f1usF73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "\n",
        "use_gpu = torch.cuda.is_available()\n",
        "print('use_gpu : ', use_gpu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8s93K3D5sF75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# wakati_corpus.txtから一部データを読み込む\n",
        "class Corpus(object):\n",
        "    def __init__(self, sp_loaded, dpaths):\n",
        "        self.sp = sp_loaded\n",
        "\n",
        "        self.train = self.load_from_file(dpaths['train'])\n",
        "        self.valid = self.load_from_file(dpaths['val'])\n",
        "        self.test = self.load_from_file(dpaths['test'])\n",
        "\n",
        "        self.train = self.assign_ids(self.train)\n",
        "        self.valid = self.assign_ids(self.valid)\n",
        "        self.test = self.assign_ids(self.test)\n",
        "    \n",
        "    def load_from_file(self, filename):\n",
        "        with open(filename, 'r', encoding='utf-8') as f:\n",
        "            return f.read().split('\\n')\n",
        "    \n",
        "    def assign_ids(self, texts):\n",
        "        tokens = []\n",
        "        for text in texts:\n",
        "            tokens.extend(self.sp.EncodeAsIds(text))\n",
        "        \n",
        "        ids = torch.from_numpy(np.array(tokens, dtype=np.int64))\n",
        "        return ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQud0SV9S10U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# コーパスを実際に読み込む\n",
        "corpus = Corpus()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}